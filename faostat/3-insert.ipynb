{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inserting the CSVs into the database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dependencies and globals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "\n",
    "from db import connection\n",
    "from db_utils import DBUtils\n",
    "\n",
    "# ID of user who imported the data\n",
    "USER_ID = 29\n",
    "\n",
    "# Dataset namespace\n",
    "NAMESPACE = 'faostat_2020'\n",
    "\n",
    "OUTPUT_PATH = 'output/'\n",
    "STANDARDIZATION_PATH = 'standardization/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load datasets, entities, variables & sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = pd.read_csv(\n",
    "    os.path.join(STANDARDIZATION_PATH, './entities_standardized.csv'), \n",
    "    index_col='name'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_entity_id_by_name = { \n",
    "    row.name: int(row['db_entity_id']) for _, row in entities.iterrows() \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We replace nan's with \"\" (empty string) because every column is a string type, \n",
    "# there should be no numeric values\n",
    "variables = pd.read_csv(os.path.join(OUTPUT_PATH, 'variables.csv')).fillna(\"\")\n",
    "datasets = pd.read_csv(os.path.join(OUTPUT_PATH, 'datasets.csv')).fillna(\"\")\n",
    "sources = pd.read_csv(os.path.join(OUTPUT_PATH, 'sources.csv')).fillna(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integrity checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_err(*args, **kwargs):\n",
    "    print(*args, file=sys.stderr, **kwargs)\n",
    "\n",
    "def assert_unique(df, subset, message=\"Duplicate row found\"):\n",
    "    duplicate_mask = df.duplicated(subset=subset)\n",
    "    if duplicate_mask.any() == True:\n",
    "        print_err(message)\n",
    "        print_err(df[duplicate_mask])\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running integrity checks...\n"
     ]
    }
   ],
   "source": [
    "print(\"Running integrity checks...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 14048/14048 [02:59<00:00, 78.37it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Integrity checks passed.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "errors = 0\n",
    "\n",
    "# Dataset IDs should be unique\n",
    "errors += assert_unique(datasets, ['id'])\n",
    "\n",
    "# Dataset names should be unique\n",
    "errors += assert_unique(datasets, ['name'])\n",
    "\n",
    "# Variable names should be unique\n",
    "errors += assert_unique(variables, ['name'])\n",
    "\n",
    "# Variable codes should be unique\n",
    "errors += assert_unique(variables, ['code'])\n",
    "\n",
    "# all entities should have a db_entity_id\n",
    "if entities['db_entity_id'].isnull().any() == True:\n",
    "    print_err(\"Entities are missing database ID\")\n",
    "    print_err(entities[entities['db_entity_id'].isnull()])\n",
    "    errors += 1\n",
    "\n",
    "# all entities in the data should exist in standardization file\n",
    "for filepath in tqdm(sorted(glob(os.path.join(OUTPUT_PATH, 'datapoints/*.csv')))):\n",
    "    df = pd.read_csv(filepath)\n",
    "    # UNIQUE (entity, year) constraint\n",
    "    errors += assert_unique(df, ['entity', 'year'], \"Duplicate row in %s\" % filepath)\n",
    "    # No empty values\n",
    "    if df['value'].isnull().any():\n",
    "        print(\"%s contains empty values in 'value' column\" % filepath)\n",
    "        errors += 1\n",
    "    # No non-numeric values\n",
    "    if not df['value'].map(np.isreal).all():\n",
    "        print(\"Non-numeric values in %s\" % filepath)\n",
    "        print(df[pd.to_numeric(df['value'], errors='coerce').isnull()])\n",
    "        errors += 1\n",
    "\n",
    "if errors != 0:\n",
    "    print_err(\"\\nIntegrity checks failed. There were %s errors.\\n\" % str(errors))\n",
    "    sys.exit(1)\n",
    "else:\n",
    "    print(\"\\nIntegrity checks passed.\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Insert database rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'connection' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-13248ca2b4a9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mconnection\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mc\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mdb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDBUtils\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatasets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'connection' is not defined"
     ]
    }
   ],
   "source": [
    "with connection as c:\n",
    "    db = DBUtils(c)\n",
    "    \n",
    "    for _, dataset in tqdm(datasets.iterrows(), total=len(datasets)):\n",
    "        \n",
    "        # Insert the dataset\n",
    "        print(\"Inserting dataset: %s\" % dataset['name'])\n",
    "        db_dataset_id = db.upsert_dataset(\n",
    "            name=dataset['name'],\n",
    "            description=dataset['description'],\n",
    "            namespace=NAMESPACE, \n",
    "            user_id=USER_ID)\n",
    "        \n",
    "        # Insert the source\n",
    "        source = sources[sources['dataset_id'] == dataset.id].iloc[0]\n",
    "        print(\"Inserting source: %s\" % source['name'])\n",
    "        db_source_id = db.upsert_source(\n",
    "            name=source['name'], \n",
    "            description=source['description'], \n",
    "            dataset_id=db_dataset_id)\n",
    "        \n",
    "        # Insert variables associated with this dataset\n",
    "        for j, variable in variables[variables.dataset_id == dataset['id']].iterrows():\n",
    "            # insert row in variables table\n",
    "            print(\"Inserting variable: %s\" % variable['name'])\n",
    "            db_variable_id = db.upsert_variable(\n",
    "                name=variable['name'], \n",
    "                code=variable['code'], \n",
    "                unit=variable['unit'], \n",
    "                description=variable['description'],\n",
    "                short_unit=None, \n",
    "                source_id=db_source_id, \n",
    "                dataset_id=db_dataset_id)\n",
    "\n",
    "            # read datapoints\n",
    "            data_values = pd.read_csv(os.path.join(OUTPUT_PATH, 'datapoints', '%s.csv' % variable.id))\n",
    "\n",
    "            values = [(float(row['value']), int(row['year']), db_entity_id_by_name[row['entity']], db_variable_id)\n",
    "                      for _, row in data_values.iterrows()]\n",
    "\n",
    "            print(\"Inserting values...\")\n",
    "            db.upsert_many(\"\"\"\n",
    "                INSERT INTO \n",
    "                    data_values (value, year, entityId, variableId)\n",
    "                VALUES \n",
    "                    (%s, %s, %s, %s)\n",
    "                ON DUPLICATE KEY UPDATE\n",
    "                    year = VALUES(year)\n",
    "            \"\"\", values)\n",
    "            \n",
    "            # We have a dummy ON DUPLICATE handler that updates the year which is essentially \n",
    "            # a no update operation. We do this only to avoid a duplicate key error. It occurs \n",
    "            # when FAO uses an Item Group and Item with the same name. For example, 'Eggs' is \n",
    "            # both an Item Group and a standalone Item in: \n",
    "            # Commodity Balances - Livestock and Fish Primary Equivalent\n",
    "            \n",
    "            # This is not ideal because we could be masking other duplication issues, we should \n",
    "            # ideally have the differentiation between groups and itemsin the database, but this \n",
    "            # requires effort and time, both of which are currently in short supply.\n",
    "            \n",
    "            print(\"Inserted %d values for variable\" % len(values))\n",
    "\n",
    "print(\"All done. Phew!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SQL to delete all data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```sql\n",
    "DELETE data_values\n",
    "FROM   data_values\n",
    "       INNER JOIN variables\n",
    "               ON variables.id = data_values.variableid\n",
    "       INNER JOIN sources\n",
    "               ON sources.id = variables.sourceid\n",
    "       INNER JOIN datasets\n",
    "               ON datasets.id = sources.datasetid\n",
    "WHERE  datasets.namespace = 'faostat_2020';\n",
    "\n",
    "DELETE variables\n",
    "FROM   variables\n",
    "       INNER JOIN sources\n",
    "               ON sources.id = variables.sourceid\n",
    "       INNER JOIN datasets\n",
    "               ON datasets.id = sources.datasetid\n",
    "WHERE  datasets.namespace = 'faostat_2020';\n",
    "\n",
    "DELETE sources\n",
    "FROM   sources\n",
    "       INNER JOIN datasets\n",
    "               ON datasets.id = sources.datasetid\n",
    "WHERE  datasets.namespace = 'faostat_2020';\n",
    "\n",
    "DELETE FROM datasets\n",
    "WHERE  namespace = 'faostat_2020';\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
