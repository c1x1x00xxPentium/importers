{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/aermolaev/Downloads/data_processing/\")\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from db import connection\n",
    "from db_utils import DBUtils\n",
    "import xlrd\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "excel_filename = \"data/PovStatsEXCEL.xlsx\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing csvs\n",
    "\n",
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "\n",
    "data = pd.read_excel(excel_filename, sheet_name=\"Series\")\n",
    "\n",
    "for item in data['Topic'].unique():\n",
    "    names.append(\"World Bank Poverty and Equity database - \" + item)\n",
    "    \n",
    "datasets = pd.DataFrame()\n",
    "datasets['id'] = [x for x in range(len(names))]\n",
    "datasets['name'] = names\n",
    "\n",
    "datasets.to_csv('datasets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, desc, d_ids = [], [], []\n",
    "\n",
    "for x in data['Indicator Name'].unique():\n",
    "    \n",
    "    df = data[data['Indicator Name'] == x]\n",
    "    \n",
    "    dataset_id = datasets[datasets['name'] == \"World Bank Poverty and Equity database - \" + df['Topic'].values[0]]['id'].values[0]\n",
    "    source_name = \"World Bank Poverty and Equity database: \" + x\n",
    "    \n",
    "    description = {}\n",
    "    description['dataPublishedBy'] = \"World Bank Poverty and Equity database\"\n",
    "    description['link'] = \"https://data.worldbank.org/data-catalog/poverty-and-equity-database\"\n",
    "    description['retrievedDate'] = datetime.datetime.now().strftime(\"%d-%b-%Y\")\n",
    "    description['additionalInfo'] = \"Definitions and characteristics of countries and other territories: \" + \"https://ourworldindata.org/grapher/povstats/POVSTATS_Country_info.xls\\r\\n\"\n",
    "    description['additionalInfo'] += \"Limitations and exceptions:\\n\" + df[\"Limitations and exceptions\"].values[0] + \"\\n\" if pd.notnull(df[\"Limitations and exceptions\"].values[0]) else \"\"\n",
    "    description['additionalInfo'] += \"Notes from original source:\\n\" + df[\"Notes from original source\"].values[0] + \"\\n\" if pd.notnull(df[\"Notes from original source\"].values[0]) else \"\"\n",
    "    description['additionalInfo'] += \"General comments:\\n\" + df[\"General comments\"].values[0] + \"\\n\" if pd.notnull(df[\"General comments\"].values[0]) else \"\"\n",
    "    description['additionalInfo'] += \"Statistical concept and methodology:\\n\" + df[\"Statistical concept and methodology\"].values[0] + \"\\n\" if pd.notnull(df[\"Statistical concept and methodology\"].values[0]) else \"\"\n",
    "    description['additionalInfo'] += \"Related source links:\\n\" + df[\"Related source links\"].values[0] + \"\\n\" if pd.notnull(df[\"Related source links\"].values[0]) else \"\"\n",
    "    description['additionalInfo'] += \"Other web links:\\n\" + df[\"Other web links\"].values[0] + \"\\n\" if pd.notnull(df[\"Other web links\"].values[0]) else \"\"\n",
    "    description['dataPublisherSource'] = df['Source'].values[0] \n",
    "    \n",
    "    \n",
    "    names.append(source_name)\n",
    "    desc.append(description)\n",
    "    d_ids.append(dataset_id)\n",
    "    \n",
    "res = pd.DataFrame()\n",
    "res['name'] = names\n",
    "res['description'] = desc\n",
    "res['dataset_id'] = d_ids\n",
    "res.to_csv(\"sources.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names, var_units, var_ids, var_codes = [], [], [], []\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    var_names.append(row['Indicator Name'] if pd.notnull(row['Indicator Name']) else \"\")\n",
    "    var_units.append(row['Unit of measure'] if pd.notnull(row['Unit of measure']) else \"\")\n",
    "    #var_codes[row['Series Code']] = row['Indicator Name']\n",
    "    var_codes.append(row['Series Code'])\n",
    "    dataset_id = datasets[datasets['name'] == \"World Bank Poverty and Equity database - \" + row['Topic']]['id'].values[0]\n",
    "    var_ids.append(dataset_id)\n",
    "    \n",
    "variables = pd.DataFrame()\n",
    "variables['name'] = var_names\n",
    "variables['unit'] = var_units\n",
    "variables['dataset_id'] = var_ids\n",
    "variables['id'] = [x for x in range(len(var_units))]\n",
    "variables.to_csv(\"variables.csv\", index=False)\n",
    "\n",
    "variables['Indicator Code'] = var_codes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_excel(excel_filename, sheet_name=\"Data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_country(row):\n",
    "        \n",
    "        row['country'] = row['country'].str.replace(r'\\s*[^A-Za-z\\s]*$', '')\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in data['Indicator Code'].unique():\n",
    "    try:\n",
    "        var_id = variables[variables['Indicator Code'] == x]['id'].values[0]\n",
    "\n",
    "        subdata = data[data['Indicator Code'] == x].drop([\"Indicator Code\", \"Indicator Name\", \"Country Code\"], axis=1)\n",
    "        subdata.dropna(how='all')\n",
    "        res = subdata.set_index('Country Name').transpose().T.unstack().reset_index()\n",
    "        res = res.dropna(subset=[0],how='all')\n",
    "        res.rename(columns={\"level_0\": \"year\", \"Country Name\": \"country\", 0: \"value\"}, inplace=True)\n",
    "        res = normalize_country(res)\n",
    "        res.to_csv('datapoints/datapoints_%s.csv' % str(var_id), index=False)  \n",
    "        \n",
    "    except:\n",
    "        print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get country names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:00<00:00, 550.62it/s]\n"
     ]
    }
   ],
   "source": [
    "countries = set()\n",
    "\n",
    "for x in tqdm(glob('datapoints/*.csv')):\n",
    "    \n",
    "   \n",
    "    data = pd.read_csv(x)\n",
    "    for j in data['country'].values:\n",
    "        countries.add(j)\n",
    "res = pd.DataFrame()\n",
    "res['name'] = list(countries)\n",
    "res.to_csv(\"distinct_countries.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:00, 551.50it/s]\n",
      "64it [00:00, 610.73it/s]\n",
      "64it [00:00, 844.04it/s]\n",
      "100%|██████████| 64/64 [00:36<00:00,  1.75it/s]\n"
     ]
    }
   ],
   "source": [
    "with connection as c:\n",
    "    db = DBUtils(c)\n",
    "    \n",
    "    entities = pd.read_csv(\"distinct_countries_standardized.csv\")\n",
    "    datasets = pd.read_csv(\"datasets.csv\")\n",
    "    sources = pd.read_csv(\"sources.csv\")\n",
    "    variables = pd.read_csv('variables.csv')\n",
    "    \n",
    "    new_entities = entities[entities['db_entity_id'].isnull()]\n",
    "    for _, entity in new_entities.iterrows():\n",
    "        entity_id = entity.name\n",
    "        entity_name = entity['name']\n",
    "        db_entity_id = db.get_or_create_entity(entity_name)\n",
    "        entities.loc[entity_id, 'db_entity_id'] = db_entity_id\n",
    "    \n",
    "    # upsert datasets\n",
    "    dataset_name_ids = {}\n",
    "    for i, row in tqdm(datasets.iterrows()):\n",
    "        dataset_id = db.upsert_dataset(name=row['name'], namespace=\"unwpp\", user_id=15)\n",
    "        dataset_name_ids[row['name']] = dataset_id\n",
    "        \n",
    "        \n",
    "    # upsert sources\n",
    "    \n",
    "    dataset_to_source_ids = {}\n",
    "    for i, row in tqdm(sources.iterrows()):\n",
    "\n",
    "        dataset_name = datasets[datasets['id'] == row['dataset_id']]['name'].values[0]\n",
    "        source_id = db.upsert_source(name=row['name'], description=json.dumps(row['description']), dataset_id=dataset_name_ids[dataset_name])\n",
    "\n",
    "        dataset_to_source_ids[dataset_name] = source_id\n",
    "\n",
    "        \n",
    "    # upsert variables\n",
    "    names_to_ids = {}\n",
    "    for i, row in tqdm(variables.iterrows()):\n",
    "        \n",
    "        dataset_name = datasets[datasets['id'] == row['dataset_id']]['name'].values[0]\n",
    "        dataset_id = dataset_name_ids[dataset_name]\n",
    "        source_id = dataset_to_source_ids[dataset_name]\n",
    "        \n",
    "        unit = row['unit'] if pd.notnull(row['unit']) else \"\"\n",
    "        \n",
    "        variable_id = db.upsert_variable(\n",
    "                                        name=row['name'], \n",
    "                                        code=None, \n",
    "                                        unit=unit, \n",
    "                                        short_unit=None, \n",
    "                                        source_id=source_id, \n",
    "                                        dataset_id=dataset_id, \n",
    "                                        description=None, \n",
    "                                        timespan='', \n",
    "                                        coverage='', \n",
    "                                        display={}\n",
    "                                        )\n",
    "        names_to_ids[row['name']] = variable_id\n",
    "        \n",
    "    #Inserting datapoints\n",
    "\n",
    "\n",
    "    datapoints_files = glob(\"datapoints/*.csv\")\n",
    "    for x in tqdm(datapoints_files): \n",
    "        # to get variable is\n",
    "        v_id = int(x.split(\"_\")[1].split(\".\")[0])\n",
    "       \n",
    "        # to get variable name\n",
    "        variable_name = variables[variables['id']==v_id]['name'].values[0]\n",
    "       \n",
    "        # to get variable id from db\n",
    "        variable_id = names_to_ids[variable_name]\n",
    "        data = pd.read_csv(x)\n",
    "\n",
    "        for i, row in data.iterrows():\n",
    "            entity_id = entities[entities['name'] == row['country']]['db_entity_id'].values[0]\n",
    "\n",
    "            year = row['year']\n",
    "            val = row['value']\n",
    "\n",
    "            db.upsert_one(\"\"\"\n",
    "                INSERT INTO data_values\n",
    "                    (value, year, entityId, variableId)\n",
    "                VALUES\n",
    "                    (%s, %s, %s, %s)\n",
    "                ON DUPLICATE KEY UPDATE\n",
    "                    value = VALUES(value),\n",
    "                    year = VALUES(year),\n",
    "                    entityId = VALUES(entityId),\n",
    "                    variableId = VALUES(variableId)\n",
    "            \"\"\", [val, int(year), str(int(entity_id)), str(variable_id)])\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
