{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pymysql\n",
    "\n",
    "# Connect to the database\n",
    "connection = pymysql.connect(db=\"\",\n",
    "                             host=\"\",\n",
    "                             port=3306,\n",
    "                             user=\"\",\n",
    "                             password='',\n",
    "                             charset='utf8mb4',\n",
    "                             autocommit=False) # requires .commit(), so everything is implicitly a transaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import time\n",
    "from unidecode import unidecode\n",
    "\n",
    "UNMODIFIED = 0\n",
    "INSERT = 1\n",
    "UPDATE = 2\n",
    "\n",
    "def normalize_entity_name(entity_name):\n",
    "    return unidecode(entity_name.lower().strip())\n",
    "\n",
    "class NotOne(ValueError):\n",
    "    pass\n",
    "\n",
    "class DBUtils:\n",
    "\n",
    "    # TODO create bulk inserts for every create? what type should they return?\n",
    "\n",
    "    def __init__(self, cursor):\n",
    "        self.cursor = cursor\n",
    "        self.counts = {\n",
    "            'tags_inserted': 0,\n",
    "            'tags_updated': 0,\n",
    "            'entities_inserted': 0,\n",
    "            'datasets_inserted': 0,\n",
    "            'datasets_updated': 0,\n",
    "            'variables_inserted': 0,\n",
    "            'variables_updated': 0,\n",
    "            'sources_inserted': 0,\n",
    "            'sources_updated': 0\n",
    "        }\n",
    "        self.entity_id_by_normalised_name = {}\n",
    "\n",
    "    def get_counts(self):\n",
    "        return self.counts\n",
    "\n",
    "    def get_entity_cache(self):\n",
    "        return self.entity_id_by_normalised_name\n",
    "\n",
    "    def fetch_one_or_none(self, *args, **kwargs):\n",
    "        self.cursor.execute(*args, **kwargs)\n",
    "        rows = self.cursor.fetchall()\n",
    "        if len(rows) > 1:\n",
    "            raise NotOne('Expected 1 or 0 rows but received %d' % (len(rows)))\n",
    "        elif len(rows) == 1:\n",
    "            return rows[0]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def fetch_one(self, *args, **kwargs):\n",
    "        result = self.fetch_one_or_none(*args, **kwargs)\n",
    "        if result is None:\n",
    "            raise NotOne('Expected 1 row but received 0')\n",
    "        else:\n",
    "            return result\n",
    "\n",
    "    def fetch_many(self, *args, **kwargs):\n",
    "        self.cursor.execute(*args, **kwargs)\n",
    "        return self.cursor.fetchall()\n",
    "\n",
    "    def insert_one(self, *args, **kwargs):\n",
    "        self.cursor.execute(*args, **kwargs)\n",
    "        return self.cursor.lastrowid\n",
    "\n",
    "    def upsert_one(self, *args, **kwargs):\n",
    "        self.cursor.execute(*args, **kwargs)\n",
    "        if self.cursor.rowcount == 0: return UNMODIFIED\n",
    "        if self.cursor.rowcount == 1: return INSERT\n",
    "        if self.cursor.rowcount == 2: return UPDATE\n",
    "        return None\n",
    "\n",
    "    def upsert_many(self, query, tuples):\n",
    "        self.cursor.executemany(query, tuples)\n",
    "\n",
    "    def execute_until_empty(self, *args, **kwargs):\n",
    "        first = True\n",
    "        while first or self.cursor.rowcount > 0:\n",
    "            first = False\n",
    "            self.cursor.execute(*args, **kwargs)\n",
    "\n",
    "    def __fetch_parent_tag(self, name):\n",
    "        (tag_id,) = self.fetch_one(\"\"\"\n",
    "            SELECT id FROM tags\n",
    "            WHERE name = %s\n",
    "            AND isBulkImport = TRUE\n",
    "            AND parentId IS NULL\n",
    "            LIMIT 1\n",
    "        \"\"\", [name])\n",
    "        return tag_id\n",
    "\n",
    "    def upsert_parent_tag(self, name):\n",
    "        try:\n",
    "            return self.__fetch_parent_tag(name)\n",
    "        except NotOne:\n",
    "            self.upsert_one(\"\"\"\n",
    "                INSERT INTO tags (name, createdAt, updatedAt, isBulkImport)\n",
    "                VALUES (%s, NOW(), NOW(), TRUE)\n",
    "            \"\"\", [name])\n",
    "            self.counts['tags_inserted'] += 1\n",
    "            return self.__fetch_parent_tag(name)\n",
    "\n",
    "    def upsert_tag(self, name, parent_id):\n",
    "        operation = self.upsert_one(\"\"\"\n",
    "            INSERT INTO\n",
    "                tags (name, parentId, createdAt, updatedAt, isBulkImport)\n",
    "            VALUES\n",
    "                (%s, %s, NOW(), NOW(), TRUE)\n",
    "            ON DUPLICATE KEY UPDATE\n",
    "                updatedAt = VALUES(updatedAt),\n",
    "                isBulkImport = VALUES(isBulkImport)\n",
    "        \"\"\", [name, parent_id])\n",
    "\n",
    "        if operation == INSERT:\n",
    "            self.counts['tags_inserted'] += 1\n",
    "        elif operation == UPDATE:\n",
    "            self.counts['tags_updated'] += 1\n",
    "\n",
    "        (tag_id,) = self.fetch_one(\"\"\"\n",
    "            SELECT id FROM tags\n",
    "            WHERE name = %s\n",
    "            AND parentId = %s\n",
    "        \"\"\", [name, parent_id])\n",
    "\n",
    "        return tag_id\n",
    "\n",
    "    def associate_dataset_tag(self, dataset_id, tag_id):\n",
    "        self.upsert_one(\"\"\"\n",
    "            INSERT INTO dataset_tags\n",
    "                (datasetId, tagId)\n",
    "            VALUES\n",
    "                (%s, %s)\n",
    "            ON DUPLICATE KEY UPDATE\n",
    "                tagId = VALUES(tagId)\n",
    "        \"\"\", [dataset_id, tag_id])\n",
    "        # ON DUPLICATE here only avoids error, it intentionally updates nothing\n",
    "\n",
    "    def upsert_dataset(self, name, namespace, user_id, tag_id=None, description='This is a dataset imported by the automated fetcher'):\n",
    "        operation = self.upsert_one(\"\"\"\n",
    "            INSERT INTO datasets\n",
    "                (name, description, namespace, createdAt, createdByUserId, updatedAt, metadataEditedAt, metadataEditedByUserId, dataEditedAt, dataEditedByUserId)\n",
    "            VALUES\n",
    "                (%s, %s, %s, NOW(), %s, NOW(), NOW(), %s, NOW(), %s)\n",
    "            ON DUPLICATE KEY UPDATE\n",
    "                name = VALUES(name),\n",
    "                description = VALUES(description),\n",
    "                namespace = VALUES(namespace),\n",
    "                updatedAt = VALUES(updatedAt),\n",
    "                metadataEditedAt = VALUES(metadataEditedAt),\n",
    "                metadataEditedByUserId = VALUES(metadataEditedByUserId),\n",
    "                dataEditedAt = VALUES(dataEditedAt),\n",
    "                dataEditedByUserId = VALUES(dataEditedByUserId)\n",
    "        \"\"\", [name, description, namespace, user_id, user_id, user_id])\n",
    "        (dataset_id,) = self.fetch_one(\"\"\"\n",
    "            SELECT id FROM datasets\n",
    "            WHERE name = %s\n",
    "            AND namespace = %s\n",
    "        \"\"\", [name, namespace])\n",
    "\n",
    "        if operation == INSERT: self.counts['datasets_inserted'] += 1\n",
    "        if operation == UPDATE: self.counts['datasets_updated'] += 1\n",
    "\n",
    "        if tag_id is not None:\n",
    "            self.associate_dataset_tag(dataset_id, tag_id)\n",
    "\n",
    "        return dataset_id\n",
    "\n",
    "    def upsert_source(self, name, description, dataset_id):\n",
    "        # There is no UNIQUE key constraint we can rely on to prevent duplicates\n",
    "        # so we have to do a SELECT before INSERT...\n",
    "        row = self.fetch_one_or_none(\"\"\"\n",
    "            SELECT id FROM sources\n",
    "            WHERE name = %s\n",
    "            AND datasetId = %s\n",
    "            LIMIT 1\n",
    "        \"\"\", [name, dataset_id])\n",
    "\n",
    "        if row is None:\n",
    "            self.upsert_one(\"\"\"\n",
    "                INSERT INTO sources\n",
    "                    (name, description, datasetId, createdAt, updatedAt)\n",
    "                VALUES\n",
    "                    (%s, %s, %s, NOW(), NOW())\n",
    "            \"\"\", [name, description, dataset_id])\n",
    "            self.counts['sources_inserted'] += 1\n",
    "            row = self.fetch_one(\"\"\"\n",
    "                SELECT id FROM sources\n",
    "                WHERE name = %s\n",
    "                AND datasetId = %s\n",
    "            \"\"\", [name, dataset_id])\n",
    "        else:\n",
    "            self.cursor.execute(\"\"\"\n",
    "                UPDATE sources\n",
    "                SET updatedAt = NOW(),\n",
    "                    description = %(description)s\n",
    "                WHERE id = %(id)s\n",
    "            \"\"\", {\n",
    "                'description': description,\n",
    "                'id': row[0]\n",
    "            })\n",
    "            self.counts['sources_updated'] += 1\n",
    "        return row[0]\n",
    "\n",
    "    def upsert_variable(self, name, code, unit, short_unit, source_id, dataset_id, description=None, timespan='', coverage='', display={}):\n",
    "        operation = self.upsert_one(\"\"\"\n",
    "            INSERT INTO variables\n",
    "                (name, code, description, unit, shortUnit, timespan, coverage, display, sourceId, datasetId, createdAt, updatedAt)\n",
    "            VALUES\n",
    "                (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW(), NOW())\n",
    "            ON DUPLICATE KEY UPDATE\n",
    "                name = VALUES(name),\n",
    "                code = VALUES(code),\n",
    "                timespan = VALUES(timespan),\n",
    "                datasetId = VALUES(datasetId),\n",
    "                sourceId = VALUES(sourceId),\n",
    "                updatedAt = VALUES(updatedAt)\n",
    "        \"\"\", [name, code, description, unit, short_unit, timespan, coverage, json.dumps(display), source_id, dataset_id])\n",
    "\n",
    "        if operation == INSERT:\n",
    "            self.counts['variables_inserted'] += 1\n",
    "        elif operation == UPDATE:\n",
    "            self.counts['variables_updated'] += 1\n",
    "\n",
    "        (var_id,) = self.fetch_one(\"\"\"\n",
    "            SELECT id FROM variables\n",
    "            WHERE (name = %s OR code = %s)\n",
    "            AND datasetId = %s\n",
    "            AND sourceId = %s\n",
    "        \"\"\", [name, code, dataset_id, source_id])\n",
    "\n",
    "        return var_id\n",
    "\n",
    "    def touch_variable(self, var_id):\n",
    "        self.cursor.execute(\"\"\"\n",
    "            UPDATE variables\n",
    "            SET updatedAt = NOW()\n",
    "            WHERE id = %s\n",
    "        \"\"\", [var_id])\n",
    "        self.counts['variables_updated'] += self.cursor.rowcount\n",
    "\n",
    "    def note_import(self, import_type, import_notes, import_state):\n",
    "        self.upsert_one(\"\"\"\n",
    "            INSERT INTO importer_importhistory (import_type, import_time, import_notes, import_state)\n",
    "            VALUES (%s, NOW(), %s, %s)\n",
    "        \"\"\", [import_type, import_notes, import_state])\n",
    "\n",
    "\n",
    "    def __get_cached_entity_id(self, name):\n",
    "        normalised_name = normalize_entity_name(name)\n",
    "        if normalised_name in self.entity_id_by_normalised_name:\n",
    "            return self.entity_id_by_normalised_name[normalised_name]\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_or_create_entity(self, name):\n",
    "        # Serve from cache if available\n",
    "        entity_id = self.__get_cached_entity_id(name)\n",
    "        if entity_id is not None:\n",
    "            return entity_id\n",
    "        # Populate cache from database\n",
    "        self.prefill_entity_cache([name])\n",
    "        entity_id = self.__get_cached_entity_id(name)\n",
    "        if entity_id is not None:\n",
    "            return entity_id\n",
    "        # If still not in cache, it's a new entity and we have to insert it\n",
    "        else:\n",
    "            self.upsert_one(\"\"\"\n",
    "                INSERT INTO entities\n",
    "                    (name, displayName, validated, createdAt, updatedAt)\n",
    "                VALUES\n",
    "                    (%s, '', FALSE, NOW(), NOW())\n",
    "            \"\"\", [name])\n",
    "            self.counts['entities_inserted'] += 1\n",
    "            (entity_id,) = self.fetch_one(\"\"\"\n",
    "                SELECT id FROM entities\n",
    "                WHERE name = %s\n",
    "            \"\"\", [name])\n",
    "            # Cache the newly created entity\n",
    "            self.entity_id_by_normalised_name[normalize_entity_name(name)] = entity_id\n",
    "            return entity_id\n",
    "\n",
    "    def prefill_entity_cache(self, names):\n",
    "        rows = self.fetch_many(\"\"\"\n",
    "            SELECT\n",
    "                LOWER(country_name),\n",
    "                LOWER(entities.name),\n",
    "                entities.id AS id\n",
    "            FROM entities\n",
    "            LEFT JOIN\n",
    "                country_name_tool_countrydata\n",
    "                ON country_name_tool_countrydata.owid_name = entities.name\n",
    "            LEFT JOIN\n",
    "                country_name_tool_countryname\n",
    "                ON country_name_tool_countryname.owid_country = country_name_tool_countrydata.id\n",
    "            WHERE\n",
    "                LOWER(country_name) IN %(country_names)s\n",
    "                OR LOWER(entities.name) IN %(country_names)s\n",
    "            ORDER BY entities.id ASC\n",
    "        \"\"\", {\n",
    "            'country_names': [normalize_entity_name(x) for x in names]\n",
    "        })\n",
    "        # Merge the two dicts\n",
    "        self.entity_id_by_normalised_name.update({\n",
    "            # entityName → entityId\n",
    "            **dict((row[1], row[2]) for row in rows if row[1]),\n",
    "            # country_tool_name → entityId\n",
    "            # the country tool name should take precedence\n",
    "            **dict((row[0], row[2]) for row in rows if row[0])\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entities\n",
    "\n",
    "Populate missing entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = pd.read_csv('./countries-standardized.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87354 Other North Africa\n",
      "87355 Total Middle East & Africa\n",
      "87356 Total Africa\n",
      "87357 Total Europe\n",
      "87358 Netherlands TTF\n",
      "87359 Japan Korea Marker\n",
      "87360 Total North America\n",
      "87361 Other S. & Cent. America\n",
      "87362 Other South America\n",
      "87363 Total Asia Pacific\n",
      "87364 Total S. & Cent. America\n",
      "87365 Other Caribbean\n",
      "35441 Rest of World\n",
      "87366 Other Southern Africa\n",
      "87367 Total Middle East\n",
      "87368 Other Northern Africa\n",
      "87369 Total CIS\n"
     ]
    }
   ],
   "source": [
    "with connection as c:\n",
    "    db = DBUtils(c)\n",
    "    new_entities = entities[entities['db_entity_id'].isnull()]\n",
    "    for _, entity in new_entities.iterrows():\n",
    "        entity_id = entity.name\n",
    "        entity_name = entity['name']\n",
    "        db_entity_id = db.get_or_create_entity(entity_name)\n",
    "        entities.loc[entity_id, 'db_entity_id'] = db_entity_id\n",
    "        print(db_entity_id, entity['name'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(entities[entities['db_entity_id'].isnull()])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "Insert a single dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection as c:\n",
    "    db = DBUtils(c)\n",
    "    dataset_id = db.upsert_dataset(\n",
    "                    \"BP Statistical Review of Global Energy\", \n",
    "                    \"bpstatreview_2019\", \n",
    "                    35, \n",
    "                    tag_id=None, \n",
    "                    description=''\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources\n",
    "Insert a single source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "with connection as c:\n",
    "    db = DBUtils(c)\n",
    "    source_id = db.upsert_source(\"BP Statistical Review of Global Energy\", description=\"\", dataset_id=dataset_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variables\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "variables = pd.read_csv(\"variables.csv\")\n",
    "names_to_ids = {}\n",
    "with connection as c:\n",
    "    db = DBUtils(c)\n",
    "    for i, row in variables.iterrows():\n",
    "        desc = row['notes'] if pd.notnull(row['notes']) else \"\"\n",
    "        variable_id = db.upsert_variable(\n",
    "                                        name=row['name'], \n",
    "                                        code=None, \n",
    "                                        unit=row['unit'], \n",
    "                                        short_unit=None, \n",
    "                                        source_id=dataset_id, \n",
    "                                        dataset_id=dataset_id, \n",
    "                                        description=desc, \n",
    "                                        timespan='', \n",
    "                                        coverage='', \n",
    "                                        display={}\n",
    "                                        )\n",
    "        names_to_ids[row['name']] = variable_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Biofuels Production - Kboed': 105298,\n",
       " 'Biofuels Production - Ktoe': 105299,\n",
       " 'Carbon Dioxide Emissions': 105300,\n",
       " 'Coal - Prices': 105301,\n",
       " 'Coal - Reserves - Anthracite and bituminous': 105302,\n",
       " 'Coal - Reserves - Sub-bituminous and lignite': 105303,\n",
       " 'Coal - Reserves - Total': 105304,\n",
       " 'Coal Consumption - Mtoe': 105305,\n",
       " 'Coal Production - Mtoe': 105306,\n",
       " 'Coal Production - Tonnes': 105307,\n",
       " 'Electricity Generation ': 105308,\n",
       " 'Gas - Prices ': 105309,\n",
       " 'Gas - Proved reserves': 105310,\n",
       " 'Gas - Proved reserves history ': 105311,\n",
       " 'Gas Consumption - Bcf': 105312,\n",
       " 'Gas Consumption - Bcm': 105313,\n",
       " 'Gas Consumption - Mtoe': 105314,\n",
       " 'Gas Production - Bcf': 105315,\n",
       " 'Gas Production - Bcm': 105316,\n",
       " 'Gas Production - Mtoe': 105317,\n",
       " 'Geo Biomass Other - Mtoe': 105318,\n",
       " 'Geo Biomass Other - TWh': 105319,\n",
       " 'Geothermal Capacity': 105320,\n",
       " 'Hydro Consumption - Mtoe': 105321,\n",
       " 'Hydro Generation - TWh': 105322,\n",
       " 'Nuclear Consumption - Mtoe': 105323,\n",
       " 'Nuclear Generation - TWh': 105324,\n",
       " 'Oil - Proved reserves': 105325,\n",
       " 'Oil - Proved reserves history': 105326,\n",
       " 'Oil - Refinery throughput': 105327,\n",
       " 'Oil - Refining capacity': 105328,\n",
       " 'Oil - Spot crude prices': 105329,\n",
       " 'Oil Consumption - Barrels': 105330,\n",
       " 'Oil Consumption - Tonnes': 105331,\n",
       " 'Oil Production - Barrels': 105332,\n",
       " 'Oil Production - Tonnes': 105333,\n",
       " 'Primary Energy Consumption': 105334,\n",
       " 'Renewables - Mtoe': 105335,\n",
       " 'Renewables - TWh': 105336,\n",
       " 'Solar Capacity': 105337,\n",
       " 'Solar Consumption - Mtoe': 105338,\n",
       " 'Solar Generation - TWh': 105339,\n",
       " 'Wind Capacity': 105340,\n",
       " 'Wind Consumption - Mtoe': 105341,\n",
       " 'Wind Generation - TWh ': 105342,\n",
       " 'Cobalt and Lithium - Prices - Cobalt': 105343,\n",
       " 'Cobalt and Lithium - Prices - Lithium Carbonate': 105344,\n",
       " 'Cobalt Production-Reserves - Production': 105345,\n",
       " 'Cobalt Production-Reserves - Reserves': 105346,\n",
       " 'Elec Gen by fuel - Oil': 105347,\n",
       " 'Elec Gen by fuel - Natural Gas': 105348,\n",
       " 'Elec Gen by fuel - Coal': 105349,\n",
       " 'Elec Gen by fuel - Nuclear energy': 105350,\n",
       " 'Elec Gen by fuel - Hydro electric': 105351,\n",
       " 'Elec Gen by fuel - Renewables': 105352,\n",
       " 'Elec Gen by fuel - Other #': 105353,\n",
       " 'Elec Gen by fuel - Total': 105354,\n",
       " 'Elec Gen from Coal': 105355,\n",
       " 'Elec Gen from Gas': 105356,\n",
       " 'Elec Gen from Oil': 105357,\n",
       " 'Elec Gen from Other': 105358,\n",
       " 'Graphite Production-Reserves': 105359,\n",
       " 'Lithium Production-Reserves': 105360,\n",
       " 'Oil - Crude prices since 1861': 105361,\n",
       " 'Oil Consumption - Mtoe': 105362,\n",
       " 'Primary Energy - Cons by fuel - Oil': 105363,\n",
       " 'Primary Energy - Cons by fuel - Natural Gas': 105364,\n",
       " 'Primary Energy - Cons by fuel - Coal': 105365,\n",
       " 'Primary Energy - Cons by fuel - Nuclear energy': 105366,\n",
       " 'Primary Energy - Cons by fuel - Hydro electric': 105367,\n",
       " 'Primary Energy - Cons by fuel - Renewables': 105368,\n",
       " 'Primary Energy - Cons by fuel - Other': 105369,\n",
       " 'Primary Energy - Cons by fuel - Total': 105370,\n",
       " 'Primary Energy - Cons capita': 105371,\n",
       " 'Rare Earth Production-Reserves': 105372,\n",
       " 'Renewables Generation by source - Wind': 105373,\n",
       " 'Renewables Generation by source - Solar': 105374,\n",
       " 'Renewables Generation by source - Other renewables+': 105375,\n",
       " 'Renewables Generation by source - Total': 104744}"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "names_to_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "datapoints_files = os.listdir(\"csvs/\")\n",
    "for x in datapoints_files:\n",
    "    if x == \".DS_Store\":\n",
    "        continue\n",
    "    # to get variable is\n",
    "    v_id = int(x.split(\"_\")[1].split(\".\")[0])\n",
    "    try:\n",
    "        # to get variable name\n",
    "        variable_name = variables[variables['id']==v_id]['name'].values[0]\n",
    "    except:\n",
    "        print(v_id)\n",
    "    # to get variable id from db\n",
    "    variable_id = names_to_ids[variable_name]\n",
    "    data = pd.read_csv(\"csvs/\"+x)\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "        entity_id = entities[entities['name'] == row['country']]['db_entity_id'].values[0]\n",
    "        if not pd.notnull(row['value']):\n",
    "            val = 0.0\n",
    "        else:\n",
    "            val = row['value']\n",
    "\n",
    "        year = row['year']\n",
    "       \n",
    "        db.upsert_one(\"\"\"\n",
    "            INSERT INTO data_values\n",
    "                (value, year, entityId, variableId)\n",
    "            VALUES\n",
    "                (%s, %s, %s, %s)\n",
    "            ON DUPLICATE KEY UPDATE\n",
    "                value = VALUES(value),\n",
    "                year = VALUES(year),\n",
    "                entityId = VALUES(entityId),\n",
    "                variableId = VALUES(variableId)\n",
    "        \"\"\", [val, int(year), str(int(entity_id)), str(variable_id)])\n",
    "\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
