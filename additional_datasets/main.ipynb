{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/Users/aermolaev/Downloads/data_processing/\")\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "from db import connection\n",
    "from db_utils import DBUtils\n",
    "import xlrd\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import json\n",
    "from glob import glob\n",
    "\n",
    "# povstats\n",
    "\n",
    "# excel_filename =  \"data_povstats/PovStatsEXCEL.xlsx\"\n",
    "# prefix = \"World Bank Poverty and Equity database\"\n",
    "# url = \"https://data.worldbank.org/data-catalog/poverty-and-equity-database\" \n",
    "# files_folder = \"files_povstats/\"\n",
    "# datapoints_folder = \"datapoints_povstats/\"\n",
    "\n",
    "# edstats\n",
    "\n",
    "# excel_filename = \"data_edstats/EdStatsEXCEL.xlsx\" \n",
    "# prefix = \"World Bank EdStats\"\n",
    "# url = \"https://datacatalog.worldbank.org/dataset/education-statistics\"\n",
    "# files_folder = \"files_edstats/\"\n",
    "# datapoints_folder = \"datapoints_edstats/\"\n",
    "\n",
    "# wdi\n",
    "\n",
    "excel_filename =  \"data_wdi/WDIEXCEL.xlsx\"\n",
    "prefix = \"World Bank Poverty and Equity database\"\n",
    "url = \"https://datacatalog.worldbank.org/dataset/world-development-indicators\" \n",
    "files_folder = \"files_wdi/\"\n",
    "datapoints_folder = \"datapoints_wdi/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing csvs\n",
    "\n",
    "# Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "names = []\n",
    "\n",
    "data = pd.read_excel(excel_filename, sheet_name=\"Series\")\n",
    "\n",
    "for item in data['Topic'].unique():\n",
    "    names.append(prefix + \" - \" + item)\n",
    "    \n",
    "datasets = pd.DataFrame()\n",
    "datasets['id'] = [x for x in range(len(names))]\n",
    "datasets['name'] = names\n",
    "\n",
    "datasets.to_csv(files_folder + 'datasets.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "names, desc, d_ids = [], [], []\n",
    "\n",
    "for x in data['Indicator Name'].unique():\n",
    "    \n",
    "    df = data[data['Indicator Name'] == x]\n",
    "    \n",
    "    dataset_id = datasets[datasets['name'] == prefix + \" - \" + df['Topic'].values[0]]['id'].values[0]\n",
    "    source_name = prefix + \": \" + x\n",
    "    \n",
    "    description = {}\n",
    "    description['dataPublishedBy'] = prefix\n",
    "    description['link'] = url\n",
    "    description['retrievedDate'] = datetime.datetime.now().strftime(\"%d-%b-%Y\")\n",
    "    description['additionalInfo'] = \"Definitions and characteristics of countries and other territories: \" + \"https://ourworldindata.org/grapher/povstats/POVSTATS_Country_info.xls\\r\\n\"\n",
    "    description['additionalInfo'] += \"Limitations and exceptions:\\n\" + df[\"Limitations and exceptions\"].values[0] + \"\\n\" if pd.notnull(df[\"Limitations and exceptions\"].values[0]) else \"\"\n",
    "    description['additionalInfo'] += \"Notes from original source:\\n\" + df[\"Notes from original source\"].values[0] + \"\\n\" if pd.notnull(df[\"Notes from original source\"].values[0]) else \"\"\n",
    "    description['additionalInfo'] += \"General comments:\\n\" + df[\"General comments\"].values[0] + \"\\n\" if pd.notnull(df[\"General comments\"].values[0]) else \"\"\n",
    "    description['additionalInfo'] += \"Statistical concept and methodology:\\n\" + df[\"Statistical concept and methodology\"].values[0] + \"\\n\" if pd.notnull(df[\"Statistical concept and methodology\"].values[0]) else \"\"\n",
    "    description['additionalInfo'] += \"Related source links:\\n\" + df[\"Related source links\"].values[0] + \"\\n\" if pd.notnull(df[\"Related source links\"].values[0]) else \"\"\n",
    "    description['additionalInfo'] += \"Other web links:\\n\" + df[\"Other web links\"].values[0] + \"\\n\" if pd.notnull(df[\"Other web links\"].values[0]) else \"\"\n",
    "    description['dataPublisherSource'] = df['Source'].values[0] \n",
    "    \n",
    "    \n",
    "    names.append(source_name)\n",
    "    desc.append(description)\n",
    "    d_ids.append(dataset_id)\n",
    "    \n",
    "res = pd.DataFrame()\n",
    "res['name'] = names\n",
    "res['description'] = desc\n",
    "res['dataset_id'] = d_ids\n",
    "res.to_csv(files_folder + \"sources.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_names, var_units, var_ids, var_codes = [], [], [], []\n",
    "\n",
    "for i, row in data.iterrows():\n",
    "    var_names.append(row['Indicator Name'] if pd.notnull(row['Indicator Name']) else \"\")\n",
    "    var_units.append(row['Unit of measure'] if pd.notnull(row['Unit of measure']) else \"\")\n",
    "    #var_codes[row['Series Code']] = row['Indicator Name']\n",
    "    var_codes.append(row['Series Code'].lower().strip())\n",
    "    dataset_id = datasets[datasets['name'] == prefix + \" - \" + row['Topic']]['id'].values[0]\n",
    "    var_ids.append(dataset_id)\n",
    "    \n",
    "variables = pd.DataFrame()\n",
    "variables['name'] = var_names\n",
    "variables['unit'] = var_units\n",
    "variables['dataset_id'] = var_ids\n",
    "variables['id'] = [x for x in range(len(var_units))]\n",
    "variables.to_csv(files_folder + \"variables.csv\", index=False)\n",
    "\n",
    "variables['Indicator Code'] = var_codes\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data = pd.read_excel(excel_filename, sheet_name=\"Data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_country(row):\n",
    "        \n",
    "        row['country'] = row['country'].str.replace(r'\\s*[^A-Za-z\\s]*$', '')\n",
    "        return row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in data['Indicator Code'].unique():\n",
    "    try:\n",
    "        var_id = variables[variables['Indicator Code'] == x.lower().strip()]['id'].values[0]\n",
    "\n",
    "        subdata = data[data['Indicator Code'] == x].drop([\"Indicator Code\", \"Indicator Name\", \"Country Code\"], axis=1)\n",
    "        subdata.dropna(how='all')\n",
    "        res = subdata.set_index('Country Name').transpose().T.unstack().reset_index()\n",
    "        res = res.dropna(subset=[0],how='all')\n",
    "        res.rename(columns={\"level_0\": \"year\", \"Country Name\": \"country\", 0: \"value\"}, inplace=True)\n",
    "        res = normalize_country(res)\n",
    "        res.to_csv(datapoints_folder + 'datapoints_%s.csv' % str(var_id), index=False)  \n",
    "        \n",
    "    except:\n",
    "        print(x)\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get country names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3665/3665 [00:07<00:00, 483.91it/s]\n"
     ]
    }
   ],
   "source": [
    "countries = set()\n",
    "\n",
    "for x in tqdm(glob(datapoints_folder + \"*.csv\")):\n",
    "    \n",
    "   \n",
    "    data = pd.read_csv(x)\n",
    "    for j in data['country'].values:\n",
    "        countries.add(j)\n",
    "res = pd.DataFrame()\n",
    "res['name'] = list(countries)\n",
    "res.to_csv(files_folder + \"distinct_countries.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "88it [00:00, 1189.35it/s]\n",
      "1429it [00:02, 629.61it/s]\n",
      "1429it [00:02, 670.56it/s]\n",
      "  0%|          | 3/1429 [00:08<1:07:50,  2.85s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-8e2f98c5968d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrow\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterrows\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mentity_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mentities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mentities\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'name'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'country'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'db_entity_id'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m             \u001b[0myear\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrow\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'year'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2984\u001b[0m         \u001b[0;31m# Do we have a (boolean) 1d indexer?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2986\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_bool_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2987\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2988\u001b[0m         \u001b[0;31m# We are left with two options: a single key, and a collection of keys,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_getitem_bool_array\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3038\u001b[0m         \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcheck_bool_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3039\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[1;32m   3602\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3603\u001b[0m         new_data = self._data.take(\n\u001b[0;32m-> 3604\u001b[0;31m             \u001b[0mindices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_block_manager_axis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverify\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3605\u001b[0m         )\n\u001b[1;32m   3606\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_constructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnew_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__finalize__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mtake\u001b[0;34m(self, indexer, axis, verify, convert)\u001b[0m\n\u001b[1;32m   1395\u001b[0m         \u001b[0mnew_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1396\u001b[0m         return self.reindex_indexer(\n\u001b[0;32m-> 1397\u001b[0;31m             \u001b[0mnew_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnew_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_dups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1398\u001b[0m         )\n\u001b[1;32m   1399\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36mreindex_indexer\u001b[0;34m(self, new_axis, indexer, axis, fill_value, allow_dups, copy)\u001b[0m\n\u001b[1;32m   1265\u001b[0m                     ),\n\u001b[1;32m   1266\u001b[0m                 )\n\u001b[0;32m-> 1267\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m             ]\n\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/internals/managers.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   1265\u001b[0m                     ),\n\u001b[1;32m   1266\u001b[0m                 )\n\u001b[0;32m-> 1267\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mblk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1268\u001b[0m             ]\n\u001b[1;32m   1269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/internals/blocks.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(self, indexer, axis, new_mgr_locs, fill_tuple)\u001b[0m\n\u001b[1;32m   1311\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1312\u001b[0m         new_values = algos.take_nd(\n\u001b[0;32m-> 1313\u001b[0;31m             \u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1314\u001b[0m         )\n\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/pandas/core/algorithms.py\u001b[0m in \u001b[0;36mtake_nd\u001b[0;34m(arr, indexer, axis, out, fill_value, mask_info, allow_fill)\u001b[0m\n\u001b[1;32m   1719\u001b[0m         \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmask_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmask_info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1720\u001b[0m     )\n\u001b[0;32m-> 1721\u001b[0;31m     \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1722\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1723\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mflip_order\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with connection as c:\n",
    "    db = DBUtils(c)\n",
    "    \n",
    "    entities = pd.read_csv(files_folder + \"distinct_countries_standardized.csv\")\n",
    "    datasets = pd.read_csv(files_folder + \"datasets.csv\")\n",
    "    sources = pd.read_csv(files_folder + \"sources.csv\")\n",
    "    variables = pd.read_csv(files_folder + 'variables.csv')\n",
    "    \n",
    "    new_entities = entities[entities['db_entity_id'].isnull()]\n",
    "    for _, entity in new_entities.iterrows():\n",
    "        entity_id = entity.name\n",
    "        entity_name = entity['name']\n",
    "        db_entity_id = db.get_or_create_entity(entity_name)\n",
    "        entities.loc[entity_id, 'db_entity_id'] = db_entity_id\n",
    "    \n",
    "    # upsert datasets\n",
    "    dataset_name_ids = {}\n",
    "    for i, row in tqdm(datasets.iterrows()):\n",
    "        dataset_id = db.upsert_dataset(name=row['name'], namespace=\"unwpp\", user_id=15)\n",
    "        dataset_name_ids[row['name']] = dataset_id\n",
    "        \n",
    "        \n",
    "    # upsert sources\n",
    "    \n",
    "    dataset_to_source_ids = {}\n",
    "    for i, row in tqdm(sources.iterrows()):\n",
    "\n",
    "        dataset_name = datasets[datasets['id'] == row['dataset_id']]['name'].values[0]\n",
    "        source_id = db.upsert_source(name=row['name'], description=json.dumps(row['description']), dataset_id=dataset_name_ids[dataset_name])\n",
    "\n",
    "        dataset_to_source_ids[dataset_name] = source_id\n",
    "\n",
    "        \n",
    "    # upsert variables\n",
    "    names_to_ids = {}\n",
    "    for i, row in tqdm(variables.iterrows()):\n",
    "        \n",
    "        dataset_name = datasets[datasets['id'] == row['dataset_id']]['name'].values[0]\n",
    "        dataset_id = dataset_name_ids[dataset_name]\n",
    "        source_id = dataset_to_source_ids[dataset_name]\n",
    "        \n",
    "        unit = row['unit'] if pd.notnull(row['unit']) else \"\"\n",
    "        \n",
    "        variable_id = db.upsert_variable(\n",
    "                                        name=row['name'], \n",
    "                                        code=None, \n",
    "                                        unit=unit, \n",
    "                                        short_unit=None, \n",
    "                                        source_id=source_id, \n",
    "                                        dataset_id=dataset_id, \n",
    "                                        description=None, \n",
    "                                        timespan='', \n",
    "                                        coverage='', \n",
    "                                        display={}\n",
    "                                        )\n",
    "        names_to_ids[row['name']] = variable_id\n",
    "        \n",
    "    #Inserting datapoints\n",
    "\n",
    "\n",
    "    datapoints_files = glob(datapoints_folder + \"*.csv\")\n",
    "    for x in tqdm(datapoints_files): \n",
    "        # to get variable is\n",
    "        v_id = int(x.split(\"_\")[2].split(\".\")[0])\n",
    "       \n",
    "        # to get variable name\n",
    "        variable_name = variables[variables['id']==v_id]['name'].values[0]\n",
    "       \n",
    "        # to get variable id from db\n",
    "        variable_id = names_to_ids[variable_name]\n",
    "        data = pd.read_csv(x)\n",
    "        \n",
    "        values_to_insert = []\n",
    "\n",
    "        for i, row in data.iterrows():\n",
    "            \n",
    "            entity_id = entities[entities['name'] == row['country']]['db_entity_id'].values[0]\n",
    "\n",
    "            year = row['year']\n",
    "            val = row['value']\n",
    "            values_to_insert.append((val, int(year), str(int(entity_id)), str(variable_id)))\n",
    "\n",
    "        db.upsert_many(\"\"\"\n",
    "            INSERT INTO data_values\n",
    "                (value, year, entityId, variableId)\n",
    "            VALUES\n",
    "                (%s, %s, %s, %s)\n",
    "            ON DUPLICATE KEY UPDATE\n",
    "                value = VALUES(value),\n",
    "                year = VALUES(year),\n",
    "                entityId = VALUES(entityId),\n",
    "                variableId = VALUES(variableId)\n",
    "        \"\"\", values_to_insert)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
